#!/usr/bin/env python3
"""Split Widdershins-generated HTTP API documentation into multiple files by tag.

This script takes a single http-api.md file generated by Widdershins and splits it
into separate files based on OpenAPI tags (e.g., Accounts, Users, API Keys).
It also rewrites internal links to maintain navigation between the split files.
"""

import re
import sys
from pathlib import Path


def slugify(text: str) -> str:
    """Convert heading text to a URL-friendly slug."""
    # Remove special characters and convert to lowercase
    slug = re.sub(r"[^\w\s-]", "", text.lower())
    # Replace spaces with hyphens
    return re.sub(r"[-\s]+", "-", slug).strip("-")


def extract_anchors(content: str) -> set[str]:
    """Extract all anchor IDs from HTML anchor tags in the content."""
    anchors = set()
    # Match <a id="anchor-name"></a> patterns
    for match in re.finditer(r'<a\s+id="([^"]+)"\s*></a>', content):
        anchors.add(match.group(1))
    return anchors


def parse_sections(content: str) -> tuple[str, list[tuple[str, str, str]]]:
    """Parse the markdown into header/intro and tagged sections.

    Returns:
        Tuple of (front_matter_and_intro, list of (tag_name, slug, content) tuples)
    """
    # Split by H1 headers (# Tag Name)
    lines = content.split("\n")
    intro_lines = []
    sections = []
    current_section = None
    current_content = []
    first_h1_seen = False

    for line in lines:
        h1_match = re.match(r"^#\s+(.+)$", line)

        if h1_match:
            if not first_h1_seen:
                # First H1 is the title, include it in intro
                intro_lines.append(line)
                first_h1_seen = True
            else:
                # Save previous section
                if current_section:
                    sections.append((
                        current_section,
                        slugify(current_section),
                        "\n".join(current_content)
                    ))

                # Start new section (this is a tag like "Accounts", "Users")
                current_section = h1_match.group(1)
                current_content = [line]
        elif current_section:
            # We're inside a section
            current_content.append(line)
        else:
            # Still in intro (before second H1)
            intro_lines.append(line)

    # Save last section
    if current_section:
        sections.append((
            current_section,
            slugify(current_section),
            "\n".join(current_content)
        ))

    intro = "\n".join(intro_lines)
    return intro, sections


def build_anchor_map(
    sections: list[tuple[str, str, str]]
) -> dict[str, tuple[str, str]]:
    """Build a map of anchor IDs to (filename, anchor) for link rewriting.

    Returns:
        Dict mapping anchor_id -> (target_file, anchor_in_file)
    """
    anchor_map = {}

    for tag_name, slug, content in sections:
        filename = f"{slug}.md"

        # Add the main section anchor (tag name as anchor)
        tag_anchor = slugify(tag_name)
        anchor_map[tag_anchor] = (filename, "")

        # Extract all HTML anchors in this section
        for match in re.finditer(r'<a\s+id="([^"]+)"\s*></a>', content):
            anchor_id = match.group(1)
            anchor_map[anchor_id] = (filename, anchor_id)

        # Track H2-H6 headers as anchors
        for match in re.finditer(r"^#{2,6}\s+(.+)$", content, re.MULTILINE):
            header_text = match.group(1)
            # Remove HTML tags from header text
            header_text = re.sub(r"<[^>]+>", "", header_text)
            header_anchor = slugify(header_text)
            anchor_map[header_anchor] = (filename, header_anchor)

            # Special handling for Schemas section:
            # Widdershins generates links like #schemaapikeystatus but headers are just "## APIKeyStatus"
            # So we need to map both formats
            if tag_name.lower() == "schemas":
                schema_anchor = "schema" + header_anchor
                anchor_map[schema_anchor] = (filename, header_anchor)

    return anchor_map


def rewrite_links(content: str, anchor_map: dict[str, tuple[str, str]]) -> str:
    """Rewrite internal markdown links to point to the correct files.

    Converts links like [text](#anchor) to [text](./file.md#anchor)
    """
    def replace_link(match):
        link_text = match.group(1)
        anchor = match.group(2)

        # Skip if it's not an anchor link
        if not anchor.startswith("#"):
            return match.group(0)

        # Remove the # prefix
        anchor_id = anchor[1:]

        # Look up in anchor map
        if anchor_id in anchor_map:
            target_file, target_anchor = anchor_map[anchor_id]
            if target_anchor:
                return f"[{link_text}](./{target_file}#{target_anchor})"
            else:
                return f"[{link_text}](./{target_file})"

        # If not found, keep original (might be external or same-page)
        return match.group(0)

    # Match markdown links [text](url)
    return re.sub(r"\[([^\]]+)\]\(([^)]+)\)", replace_link, content)


def generate_index(
    intro: str,
    sections: list[tuple[str, str, str]]
) -> str:
    """Generate an index.md file with links to all sections."""
    # Extract just the title from front matter if present
    title_match = re.search(r"^---\ntitle:\s*(.+)\n---\n", intro, re.MULTILINE)

    if title_match:
        title = title_match.group(1)
        # Start with clean front matter
        index_content = f"---\ntitle: {title}\n---\n\n"
        # Get content after front matter
        intro_text = intro.split("---\n", 2)[2] if intro.count("---") >= 2 else intro
    else:
        index_content = "---\ntitle: HTTP API Reference\n---\n\n"
        intro_text = intro

    index_content += intro_text.strip() + "\n\n"

    # Add section links
    index_content += "## API Sections\n\n"
    for tag_name, slug, _ in sections:
        index_content += f"- [{tag_name}](./{slug}.md)\n"

    return index_content


def split_http_api(input_file: Path, output_dir: Path) -> None:
    """Split HTTP API documentation into multiple files."""
    # Read input file
    content = input_file.read_text()

    # Parse sections
    intro, sections = parse_sections(content)

    if not sections:
        print("Warning: No sections found to split. Creating single index file.")
        output_dir.mkdir(parents=True, exist_ok=True)
        (output_dir / "index.md").write_text(intro)
        return

    # Build anchor map for link rewriting
    anchor_map = build_anchor_map(sections)

    # Create output directory
    output_dir.mkdir(parents=True, exist_ok=True)

    # Write each section to its own file
    for tag_name, slug, section_content in sections:
        # Rewrite links in this section
        rewritten_content = rewrite_links(section_content, anchor_map)

        # Write to file
        output_file = output_dir / f"{slug}.md"
        output_file.write_text(rewritten_content)
        print(f"  Created {output_file}")

    # Generate and write index
    index_content = generate_index(intro, sections)
    index_file = output_dir / "index.md"
    index_file.write_text(index_content)
    print(f"  Created {index_file}")


def main():
    """Main entry point."""
    if len(sys.argv) != 3:
        print("Usage: split_http_api.py <input_file> <output_dir>")
        sys.exit(1)

    input_file = Path(sys.argv[1])
    output_dir = Path(sys.argv[2])

    if not input_file.exists():
        print(f"Error: Input file not found: {input_file}")
        sys.exit(1)

    print(f"Splitting {input_file} into {output_dir}/")
    split_http_api(input_file, output_dir)
    print("HTTP API documentation split successfully!")


if __name__ == "__main__":
    main()
